{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7163213,"sourceType":"datasetVersion","datasetId":4115721},{"sourceId":8394414,"sourceType":"datasetVersion","datasetId":4993760}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Automatidata project - Machine learning model**","metadata":{"id":"DtNBZFHO3M7n"}},{"cell_type":"markdown","source":"You are a data professional in a data consulting firm called Automatidata. Their client, the New York City Taxi & Limousine Commission (New York City TLC), was impressed with the work you have done and has requested that you build a machine learning model to predict if a customer will not leave a tip. They want to use the model in an app that will alert taxi drivers to customers who are unlikely to tip, since drivers depend on tips.\n","metadata":{"id":"9ttxbfHXzB4e"}},{"cell_type":"markdown","source":"# Build a machine learning model \n\nIn this activity, you will practice using tree-based modeling techniques to predict on a binary target class.  \n\n**The purpose** of this model is to find ways to generate more revenue for taxi cab drivers.  \n  \n**The goal** of this model is to predict whether or not a customer is a generous tipper.  \n\n\n*This activity has three parts:*\n\n**Part 1:** Ethical considerations \n* Consider the ethical implications of the request \n\n* Should the objective of the model be adjusted?\n\n**Part 2:** Feature engineering\n\n* Perform feature selection, extraction, and transformation to prepare the data for modeling\n\n**Part 3:** Modeling\n\n* Build the models, evaluate them, and advise on next steps\n\n\n","metadata":{"id":"rgSbVJvomcVa"}},{"cell_type":"markdown","source":"# Build a machine learning model","metadata":{}},{"cell_type":"markdown","source":"## PACE: Plan \n\nIn this stage, consider the following questions:\n\n1.   What are you being asked to do?\n\n\n2.   What are the ethical implications of the model? What are the consequences of your model making errors?\n  *   What is the likely effect of the model when it predicts a false negative (i.e., when the model says a customer will give a tip, but they actually won't)?\n  \n  *   What is the likely effect of the model when it predicts a false positive (i.e., when the model says a customer will not give a tip, but they actually will)?  \n  \n  \n3.   Do the benefits of such a model outweigh the potential problems?\n  \n4.   Would you proceed with the request to build this model? Why or why not?\n \n5.   Can the objective be modified to make it less problematic?\n\n\n**Question 1:**\n\nPredict if a customer will **not** leave a tip.\n\n**Question 2:**\n\nDrivers who didn't receive tips will probably be upset that the app told them a customer would leave a tip. If it happened often, drivers might not trust the app.\nDrivers are unlikely to pick up people who are predicted to not leave tips. Customers will have difficulty finding a taxi that will pick them up, and might get angry at the taxi company. Even when the model is correct, people who can't afford to tip will find it more difficult to get taxis, which limits the accessibility of taxi service to those who pay extra.\n\n**Question 3:**\n\nIt's not good to disincentivize drivers from picking up customers. It could also cause a customer backlash. The problems seem to outweigh the benefits.\n\n**Question 4:**\n\nNo. Effectively limiting equal access to taxis is ethically problematic, and carries a lot of risk.\n\n**Question 5:**\n\nWe can build a model that predicts the most generous customers. This could accomplish the goal of helping taxi drivers increase their earnings from tips while preventing the wrongful exclusion of certain people from using taxis.","metadata":{"id":"E5g1A74r0ow_"}},{"cell_type":"markdown","source":"Suppose you were to modify the modeling objective so, instead of predicting people who won't tip at all, you predicted people who are particularly generous&mdash;those who will tip 20% or more? Consider the following questions:\n\n\n**Question 1:**  What features do you need to make this prediction?\n\nIdeally, we'd have behavioral history for each customer, so we could know how much they tipped on previous taxi rides. We'd also want times, dates, and locations of both pickups and dropoffs, estimated fares, and payment method.\n\n**Question 2:**  What would be the target variable?\n\n\nThe target variable would be a binary variable (1 or 0) that indicates whether or not the customer is expected to tip â‰¥ 20%.\n\n**Question 3:**  \n\nThis is a supervised learning, classification task. We could use accuracy, precision, recall, F-score, area under the ROC curve, or a number of other metrics. However, we don't have enough information at this time to know which are most appropriate. We need to know the class balance of the target variable.","metadata":{"id":"GUUrVKTe4cc5"}},{"cell_type":"markdown","source":"### **Task 1. Imports and data loading**\n\nImport packages and libraries needed to build and evaluate random forest and XGBoost classification models.","metadata":{"id":"e8Vm3QEfGELS"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\\\nf1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# This is the function that helps plot feature importance \nfrom xgboost import plot_importance\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24780,"status":"ok","timestamp":1669031931287,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"fKhnX2Puf4Bt","outputId":"b8c8bbf8-390d-49a7-8c4a-3242114aecf8","execution":{"iopub.status.busy":"2024-05-12T19:11:43.637957Z","iopub.execute_input":"2024-05-12T19:11:43.638461Z","iopub.status.idle":"2024-05-12T19:11:43.654083Z","shell.execute_reply.started":"2024-05-12T19:11:43.638426Z","shell.execute_reply":"2024-05-12T19:11:43.652775Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/input/yellow-taxi-trip-data-2017/C2_2017_Yellow_Taxi_Trip_Data.csv\n/kaggle/input/nyc-preds-means/nyc_preds_means.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# RUN THIS CELL TO SEE ALL COLUMNS \n# This lets us see all of the columns, preventing Jupiter from redacting them.\npd.set_option('display.max_columns', None)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:11:43.656876Z","iopub.execute_input":"2024-05-12T19:11:43.657974Z","iopub.status.idle":"2024-05-12T19:11:43.668761Z","shell.execute_reply.started":"2024-05-12T19:11:43.657914Z","shell.execute_reply":"2024-05-12T19:11:43.667419Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Begin by reading in the data. There are two dataframes: one containing the original data, the other containing the mean durations, mean distances, and predicted fares from the previous course's project called nyc_preds_means.csv.\n\n**Note:** `pandas` reads in the dataset as `df0`, now inspect the first five rows. As shown in this cell, the dataset has been automatically loaded in for you. You do not need to download the .csv file, or provide more code, in order to access the dataset and proceed with this lab. Please continue with this activity by completing the following instructions.","metadata":{"id":"IeXTZ2tdbALL"}},{"cell_type":"code","source":"# Load dataset into dataframe\ndf0 = pd.read_csv('/kaggle/input/yellow-taxi-trip-data-2017/C2_2017_Yellow_Taxi_Trip_Data.csv')\n\n# Import predicted fares and mean distance and duration from previous course\nnyc_preds_means = pd.read_csv('/kaggle/input/nyc-preds-means/nyc_preds_means.csv')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":377,"status":"error","timestamp":1669031931655,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"5weTXGKqa_iG","outputId":"6c91cb73-20d1-43ce-99e1-f28a32e0d4cb","execution":{"iopub.status.busy":"2024-05-12T19:11:43.670279Z","iopub.execute_input":"2024-05-12T19:11:43.670696Z","iopub.status.idle":"2024-05-12T19:11:43.811834Z","shell.execute_reply.started":"2024-05-12T19:11:43.670664Z","shell.execute_reply":"2024-05-12T19:11:43.810411Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Inspect the first few rows of `df0`.","metadata":{}},{"cell_type":"code","source":"# Inspect the first few rows of df0\ndf0.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:11:43.813556Z","iopub.execute_input":"2024-05-12T19:11:43.813972Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  \\\n0    24870114         2   03/25/2017 8:55:43 AM   03/25/2017 9:09:47 AM   \n1    35634249         1   04/11/2017 2:53:28 PM   04/11/2017 3:19:58 PM   \n2   106203690         1   12/15/2017 7:26:56 AM   12/15/2017 7:34:08 AM   \n3    38942136         2   05/07/2017 1:17:59 PM   05/07/2017 1:48:14 PM   \n4    30841670         2  04/15/2017 11:32:20 PM  04/15/2017 11:49:03 PM   \n\n   passenger_count  trip_distance  RatecodeID store_and_fwd_flag  \\\n0                6           3.34           1                  N   \n1                1           1.80           1                  N   \n2                1           1.00           1                  N   \n3                1           3.70           1                  N   \n4                1           4.37           1                  N   \n\n   PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  \\\n0           100           231             1         13.0    0.0      0.5   \n1           186            43             1         16.0    0.0      0.5   \n2           262           236             1          6.5    0.0      0.5   \n3           188            97             1         20.5    0.0      0.5   \n4             4           112             2         16.5    0.5      0.5   \n\n   tip_amount  tolls_amount  improvement_surcharge  total_amount  \n0        2.76           0.0                    0.3         16.56  \n1        4.00           0.0                    0.3         20.80  \n2        1.45           0.0                    0.3          8.75  \n3        6.39           0.0                    0.3         27.69  \n4        0.00           0.0                    0.3         17.80  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>VendorID</th>\n      <th>tpep_pickup_datetime</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>RatecodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>PULocationID</th>\n      <th>DOLocationID</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24870114</td>\n      <td>2</td>\n      <td>03/25/2017 8:55:43 AM</td>\n      <td>03/25/2017 9:09:47 AM</td>\n      <td>6</td>\n      <td>3.34</td>\n      <td>1</td>\n      <td>N</td>\n      <td>100</td>\n      <td>231</td>\n      <td>1</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>2.76</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>16.56</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35634249</td>\n      <td>1</td>\n      <td>04/11/2017 2:53:28 PM</td>\n      <td>04/11/2017 3:19:58 PM</td>\n      <td>1</td>\n      <td>1.80</td>\n      <td>1</td>\n      <td>N</td>\n      <td>186</td>\n      <td>43</td>\n      <td>1</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>4.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>20.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106203690</td>\n      <td>1</td>\n      <td>12/15/2017 7:26:56 AM</td>\n      <td>12/15/2017 7:34:08 AM</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>N</td>\n      <td>262</td>\n      <td>236</td>\n      <td>1</td>\n      <td>6.5</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.45</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>8.75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38942136</td>\n      <td>2</td>\n      <td>05/07/2017 1:17:59 PM</td>\n      <td>05/07/2017 1:48:14 PM</td>\n      <td>1</td>\n      <td>3.70</td>\n      <td>1</td>\n      <td>N</td>\n      <td>188</td>\n      <td>97</td>\n      <td>1</td>\n      <td>20.5</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>6.39</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>27.69</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30841670</td>\n      <td>2</td>\n      <td>04/15/2017 11:32:20 PM</td>\n      <td>04/15/2017 11:49:03 PM</td>\n      <td>1</td>\n      <td>4.37</td>\n      <td>1</td>\n      <td>N</td>\n      <td>4</td>\n      <td>112</td>\n      <td>2</td>\n      <td>16.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>17.80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  \\\n0    24870114         2   03/25/2017 8:55:43 AM   03/25/2017 9:09:47 AM   \n1    35634249         1   04/11/2017 2:53:28 PM   04/11/2017 3:19:58 PM   \n2   106203690         1   12/15/2017 7:26:56 AM   12/15/2017 7:34:08 AM   \n3    38942136         2   05/07/2017 1:17:59 PM   05/07/2017 1:48:14 PM   \n4    30841670         2  04/15/2017 11:32:20 PM  04/15/2017 11:49:03 PM   \n\n   passenger_count  trip_distance  RatecodeID store_and_fwd_flag  \\\n0                6           3.34           1                  N   \n1                1           1.80           1                  N   \n2                1           1.00           1                  N   \n3                1           3.70           1                  N   \n4                1           4.37           1                  N   \n\n   PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  \\\n0           100           231             1         13.0    0.0      0.5   \n1           186            43             1         16.0    0.0      0.5   \n2           262           236             1          6.5    0.0      0.5   \n3           188            97             1         20.5    0.0      0.5   \n4             4           112             2         16.5    0.5      0.5   \n\n   tip_amount  tolls_amount  improvement_surcharge  total_amount  \n0        2.76           0.0                    0.3         16.56  \n1        4.00           0.0                    0.3         20.80  \n2        1.45           0.0                    0.3          8.75  \n3        6.39           0.0                    0.3         27.69  \n4        0.00           0.0                    0.3         17.80  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>VendorID</th>\n      <th>tpep_pickup_datetime</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>RatecodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>PULocationID</th>\n      <th>DOLocationID</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24870114</td>\n      <td>2</td>\n      <td>03/25/2017 8:55:43 AM</td>\n      <td>03/25/2017 9:09:47 AM</td>\n      <td>6</td>\n      <td>3.34</td>\n      <td>1</td>\n      <td>N</td>\n      <td>100</td>\n      <td>231</td>\n      <td>1</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>2.76</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>16.56</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35634249</td>\n      <td>1</td>\n      <td>04/11/2017 2:53:28 PM</td>\n      <td>04/11/2017 3:19:58 PM</td>\n      <td>1</td>\n      <td>1.80</td>\n      <td>1</td>\n      <td>N</td>\n      <td>186</td>\n      <td>43</td>\n      <td>1</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>4.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>20.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106203690</td>\n      <td>1</td>\n      <td>12/15/2017 7:26:56 AM</td>\n      <td>12/15/2017 7:34:08 AM</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>N</td>\n      <td>262</td>\n      <td>236</td>\n      <td>1</td>\n      <td>6.5</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.45</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>8.75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38942136</td>\n      <td>2</td>\n      <td>05/07/2017 1:17:59 PM</td>\n      <td>05/07/2017 1:48:14 PM</td>\n      <td>1</td>\n      <td>3.70</td>\n      <td>1</td>\n      <td>N</td>\n      <td>188</td>\n      <td>97</td>\n      <td>1</td>\n      <td>20.5</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>6.39</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>27.69</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30841670</td>\n      <td>2</td>\n      <td>04/15/2017 11:32:20 PM</td>\n      <td>04/15/2017 11:49:03 PM</td>\n      <td>1</td>\n      <td>4.37</td>\n      <td>1</td>\n      <td>N</td>\n      <td>4</td>\n      <td>112</td>\n      <td>2</td>\n      <td>16.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>17.80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Inspect the first few rows of `nyc_preds_means`.","metadata":{}},{"cell_type":"code","source":"# Inspect the first few rows of `nyc_preds_means`\nnyc_preds_means.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Join the two dataframes\n\nJoin the two dataframes using a method of your choice.","metadata":{}},{"cell_type":"code","source":"# Merge datasets\ndf0 = df0.merge(nyc_preds_means,\n                left_index=True,\n                right_index=True)\n\ndf0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## PACE: **Analyze**\n\nConsider the questions in your PACE Strategy Documentto reflect on the Analyze stage.","metadata":{"id":"EgPRBjizg1oo"}},{"cell_type":"markdown","source":"### **Task 2. Feature engineering**\n\nYou have already prepared much of this data and performed exploratory data analysis (EDA) in previous courses. \n\nCall `info()` on the dataframe.","metadata":{"id":"5VZowX9rhU1o"}},{"cell_type":"code","source":"df0.info()","metadata":{"executionInfo":{"elapsed":33,"status":"aborted","timestamp":1669031931656,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"mBOSW8IDbO_d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You know from your EDA that customers who pay cash generally have a tip amount of $0. To meet the modeling objective, you'll need to sample the data to select only the customers who pay with credit card. \n\nCopy `df0` and assign the result to a variable called `df1`. Then, use a Boolean mask to filter `df1` so it contains only customers who paid with credit card.","metadata":{"id":"0D2RvXk0kwsx"}},{"cell_type":"code","source":"# Subset the data to isolate only customers who paid by credit card\ndf1 = df0[df0['payment_type']==1]","metadata":{"executionInfo":{"elapsed":33,"status":"aborted","timestamp":1669031931657,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"_pmNd78plQYr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **Target**\n\nNotice that there isn't a column that indicates tip percent, which is what you need to create the target variable. You'll have to engineer it. \n\nAdd a `tip_percent` column to the dataframe by performing the following calculation:  \n<br/>  \n\n\n$$tip\\ percent = \\frac{tip\\ amount}{total\\ amount - tip\\ amount}$$  \n\nRound the result to three places beyond the decimal. **This is an important step.** It affects how many customers are labeled as generous tippers. In fact, without performing this step, approximately 1,800 people who do tip â‰¥ 20% would be labeled as not generous. \n\nTo understand why, you must consider how floats work. Computers make their calculations using floating-point arithmetic (hence the word \"float\"). Floating-point arithmetic is a system that allows computers to express both very large numbers and very small numbers with a high degree of precision, encoded in binary. However, precision is limited by the number of bits used to represent a number, which is generally 32 or 64, depending on the capabilities of your operating system. \n\nThis comes with limitations in that sometimes calculations that should result in clean, precise values end up being encoded as very long decimals. Take, for example, the following calculation:\n","metadata":{"id":"EcYudtSYyMcZ"}},{"cell_type":"code","source":"# Run this cell\n1.1 + 2.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice the three that is 16 places to the right of the decimal. As a consequence, if you were to then have a step in your code that identifies values â‰¤ 3.3, this would not be included in the result. Therefore, whenever you perform a calculation to compute a number that is then used to make an important decision or filtration, round the number. How many degrees of precision you round to is your decision, which should be based on your use case. \n\nRefer to this [guide for more information related to floating-point arithmetic](https://floating-point-gui.de/formats/fp/).  \nRefer to this [guide for more information related to fixed-point arithmetic](https://inst.eecs.berkeley.edu/~cs61c/sp06/handout/fixedpt.html), which is an alternative to floating-point arithmetic used in certain cases.","metadata":{}},{"cell_type":"code","source":"# Create tip % col\ndf1['tip_percent'] = round(df1['tip_amount'] / (df1['total_amount'] - df1['tip_amount']), 3)","metadata":{"executionInfo":{"elapsed":31,"status":"aborted","timestamp":1669031931658,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"guanzJd8zBla","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now create another column called `generous`. This will be the target variable. The column should be a binary indicator of whether or not a customer tipped â‰¥ 20% (0=no, 1=yes).\n\n1. Begin by making the `generous` column a copy of the `tip_percent` column.\n2. Reassign the column by converting it to Boolean (True/False).\n3. Reassign the column by converting Boolean to binary (1/0).","metadata":{"id":"Bqb-SWfs-8Xn"}},{"cell_type":"code","source":"# Create 'generous' col (target)\ndf1['generous'] = df1['tip_percent']\ndf1['generous'] = (df1['generous'] >= 0.2)\ndf1['generous'] = df1['generous'].astype(int)","metadata":{"executionInfo":{"elapsed":31,"status":"aborted","timestamp":1669031931658,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"nqDSe0DSGwhB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To convert from Boolean to binary, use `.astype(int)` on the column.\n","metadata":{"id":"ddLE6KE1KeF7"}},{"cell_type":"markdown","source":"##### Create day column","metadata":{"id":"EkubbZRRKrjO"}},{"cell_type":"markdown","source":"Next, you're going to be working with the pickup and dropoff columns.\n\nConvert the `tpep_pickup_datetime` and `tpep_dropoff_datetime` columns to datetime.","metadata":{"id":"H27zUVIlkaxA"}},{"cell_type":"code","source":"# Convert pickup and dropoff cols to datetime\ndf1['tpep_pickup_datetime'] = pd.to_datetime(df1['tpep_pickup_datetime'], format='%m/%d/%Y %I:%M:%S %p')\ndf1['tpep_dropoff_datetime'] = pd.to_datetime(df1['tpep_dropoff_datetime'], format='%m/%d/%Y %I:%M:%S %p')\ndf1","metadata":{"executionInfo":{"elapsed":31,"status":"aborted","timestamp":1669031931660,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"OIycxWBMkafJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a `day` column that contains only the day of the week when each passenger was picked up. Then, convert the values to lowercase.","metadata":{"id":"zpcM4FvNyPFY"}},{"cell_type":"code","source":"# Create a 'day' col\ndf1['day'] = df1['tpep_pickup_datetime'].dt.day_name().str.lower()","metadata":{"executionInfo":{"elapsed":30,"status":"aborted","timestamp":1669031931661,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"abUvtMaYyWpD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nTo convert to day name, use `dt.day_name()` on the column.\n","metadata":{"id":"RZZhKnQrQgNM"}},{"cell_type":"markdown","source":"#### Create time of day columns","metadata":{}},{"cell_type":"markdown","source":"Next, engineer four new columns that represent time of day bins. Each column should contain binary values (0=no, 1=yes) that indicate whether a trip began (picked up) during the following times:\n\n`am_rush` = [06:00&ndash;10:00)  \n`daytime` = [10:00&ndash;16:00)  \n`pm_rush` = [16:00&ndash;20:00)  \n`nighttime` = [20:00&ndash;06:00)  \n\nTo do this, first create the four columns. For now, each new column should be identical and contain the same information: the hour (only) from the `tpep_pickup_datetime` column.","metadata":{"id":"HwslVt8Hpu7x"}},{"cell_type":"code","source":"# Create 'am_rush' col\ndf1['am_rush'] = df1['tpep_pickup_datetime'].dt.hour\n\n# Create 'daytime' col\ndf1['daytime'] = df1['tpep_pickup_datetime'].dt.hour\n\n# Create 'pm_rush' col\ndf1['pm_rush'] = df1['tpep_pickup_datetime'].dt.hour\n\n# Create 'nighttime' col\ndf1['nighttime'] = df1['tpep_pickup_datetime'].dt.hour","metadata":{"executionInfo":{"elapsed":30,"status":"aborted","timestamp":1669031931662,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"x8LFySUyprau","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You'll need to write four functions to convert each new column to binary (0/1). Begin with `am_rush`. Complete the function so if the hour is between [06:00â€“10:00), it returns 1, otherwise, it returns 0.","metadata":{"id":"HDyfsTDvwORL"}},{"cell_type":"code","source":"# Define 'am_rush()' conversion function [06:00â€“10:00)\ndef am_rush(hour):\n    if 6 <= hour['am_rush'] < 10:\n        val = 1\n    else:\n        val = 0\n    return val","metadata":{"executionInfo":{"elapsed":30,"status":"aborted","timestamp":1669031931663,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"oAE4vRz0wGtN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, apply the `am_rush()` function to the `am_rush` series to perform the conversion. Print the first five values of the column to make sure it did what you expected it to do.\n\n**Note:** Be careful! If you run this cell twice, the function will be reapplied and the values will all be changed to 0.","metadata":{"id":"hHY1-6cIxfA6"}},{"cell_type":"code","source":"# Apply 'am_rush' function to the 'am_rush' series\ndf1['am_rush'] = df1.apply(am_rush, axis=1)\ndf1['am_rush'].head()","metadata":{"executionInfo":{"elapsed":29,"status":"aborted","timestamp":1669031931663,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"sWFojyk9xdDY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write functions to convert the three remaining columns and apply them to their respective series.","metadata":{"id":"mSY6SsdK0lpn"}},{"cell_type":"code","source":"# Define 'daytime()' conversion function [10:00â€“16:00)\ndef daytime(hour):\n    if 10 <= hour['daytime'] < 16:\n        val = 1\n    else:\n        val = 0\n    return val","metadata":{"executionInfo":{"elapsed":29,"status":"aborted","timestamp":1669031931664,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"UADnzaIjzwLG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply 'daytime' function to the 'daytime' series\ndf1['daytime'] = df1.apply(daytime, axis=1)","metadata":{"executionInfo":{"elapsed":29,"status":"aborted","timestamp":1669031931664,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"ReHpKxoC1Qsx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define 'pm_rush()' conversion function [16:00â€“20:00)\ndef pm_rush(hour):\n    if 16 <= hour['pm_rush'] < 20:\n        val = 1\n    else:\n        val = 0\n    return val","metadata":{"executionInfo":{"elapsed":29,"status":"aborted","timestamp":1669031931665,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"rP-ZBOHT1WQY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply 'pm_rush' function to the 'pm_rush' series\ndf1['pm_rush'] = df1.apply(pm_rush, axis=1)","metadata":{"executionInfo":{"elapsed":28,"status":"aborted","timestamp":1669031931665,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"h0zWPBqr1mX4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define 'nighttime()' conversion function [20:00â€“06:00)\ndef nighttime(hour):\n    if 20 <= hour['nighttime'] < 24:\n        val = 1\n    elif 0 <= hour['nighttime'] < 6:\n        val = 1\n    else:\n        val = 0\n    return val","metadata":{"executionInfo":{"elapsed":28,"status":"aborted","timestamp":1669031931666,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"u5O0LPLz2CSa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply 'nighttime' function to the 'nighttime' series\ndf1['nighttime'] = df1.apply(nighttime, axis=1)","metadata":{"executionInfo":{"elapsed":28,"status":"aborted","timestamp":1669031931666,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"kLGmBXkT2RTi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create month column","metadata":{}},{"cell_type":"markdown","source":"Now, create a `month` column that contains only the abbreviated name of the month when each passenger was picked up, then convert the result to lowercase.","metadata":{"id":"VrUmDy8U28bs"}},{"cell_type":"markdown","source":"Refer to the [strftime cheatsheet](https://strftime.org/) for help.\n","metadata":{"id":"bU5Zchdxgk3w"}},{"cell_type":"code","source":"# Create 'month' col\ndf1['month'] = df1['tpep_pickup_datetime'].dt.strftime('%b').str.lower()","metadata":{"executionInfo":{"elapsed":28,"status":"aborted","timestamp":1669031931668,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"rv5ZKK6-2YAh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examine the first five rows of your dataframe.","metadata":{"id":"qWbNVbngihE6"}},{"cell_type":"code","source":"df1.head()","metadata":{"executionInfo":{"elapsed":25715,"status":"aborted","timestamp":1669031931669,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"jWxemeyl4vwQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop columns\n\nDrop redundant and irrelevant columns as well as those that would not be available when the model is deployed. This includes information like payment type, trip distance, tip amount, tip percentage, total amount, toll amount, etc. The target variable (`generous`) must remain in the data because it will get isolated as the `y` data for modeling.","metadata":{}},{"cell_type":"code","source":"df1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop columns\ndrop_cols = ['Unnamed: 0', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n             'payment_type', 'trip_distance', 'store_and_fwd_flag', 'payment_type',\n             'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n             'improvement_surcharge', 'total_amount', 'tip_percent']\n\ndf1 = df1.drop(drop_cols, axis=1)\ndf1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Variable encoding","metadata":{}},{"cell_type":"markdown","source":"Many of the columns are categorical and will need to be dummied (converted to binary). Some of these columns are numeric, but they actually encode categorical information, such as `RatecodeID` and the pickup and dropoff locations. To make these columns recognizable to the `get_dummies()` function as categorical variables, you'll first need to convert them to `type(str)`. \n\n1. Define a variable called `cols_to_str`, which is a list of the numeric columns that contain categorical information and must be converted to string: `RatecodeID`, `PULocationID`, `DOLocationID`.\n2. Write a for loop that converts each column in `cols_to_str` to string.\n","metadata":{"id":"BVs01W-Iitu7"}},{"cell_type":"code","source":"# 1. Define list of cols to convert to string\ncols_to_str = ['RatecodeID', 'PULocationID', 'DOLocationID', 'VendorID']\n\n# 2. Convert each column to string\nfor col in cols_to_str:\n    df1[col] = df1[col].astype('str')","metadata":{"executionInfo":{"elapsed":25714,"status":"aborted","timestamp":1669031931670,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"FbB4AfATHqjC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nTo convert to string, use `astype(str)` on the column.\n","metadata":{"id":"6j6Nyb5RnsvC"}},{"cell_type":"markdown","source":"Now convert all the categorical columns to binary.\n\n1. Call `get_dummies()` on the dataframe and assign the results back to a new dataframe called `df2`.\n","metadata":{"id":"_5Ubw8O1pKRO"}},{"cell_type":"code","source":"# Convert categoricals to binary\ndf2 = pd.get_dummies(df1, drop_first=True)\ndf2.info()","metadata":{"executionInfo":{"elapsed":25711,"status":"aborted","timestamp":1669031931671,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"H94yLzUMHqgB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Evaluation metric\n\nBefore modeling, you must decide on an evaluation metric. \n\n1. Examine the class balance of your target variable. ","metadata":{"id":"nZfNE37b-LlJ"}},{"cell_type":"code","source":"# Get class balance of 'generous' col\ndf2['generous'].value_counts(normalize=True)","metadata":{"executionInfo":{"elapsed":25704,"status":"aborted","timestamp":1669031931672,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"4mRefXCF-K_c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A little over half of the customers in this dataset were \"generous\" (tipped â‰¥ 20%). The dataset is very nearly balanced.\n\nTo determine a metric, consider the cost of both kinds of model error:\n* False positives (the model predicts a tip â‰¥ 20%, but the customer does not give one)\n* False negatives (the model predicts a tip < 20%, but the customer gives more)\n\nFalse positives are worse for cab drivers, because they would pick up a customer expecting a good tip and then not receive one, frustrating the driver.\n\nFalse negatives are worse for customers, because a cab driver would likely pick up a different customer who was predicted to tip more&mdash;even when the original customer would have tipped generously.\n\n**The stakes are relatively even. You want to help taxi drivers make more money, but you don't want this to anger customers. Your metric should weigh both precision and recall equally. Which metric is this?**\n\n**Response:**  F<sub>1</sub> score is the metric that places equal weight on true postives and false positives, and so therefore on precision and recall.","metadata":{"id":"TjgkLrOf_OrE"}},{"cell_type":"markdown","source":"## PACE: **Construct**\n","metadata":{"id":"3n1eikFh8akS"}},{"cell_type":"markdown","source":"### **Task 3. Modeling**","metadata":{"id":"G5jzGjOS8iiv"}},{"cell_type":"markdown","source":"##### **Split the data**\n\nNow you're ready to model. The only remaining step is to split the data into features/target variable and training/testing data. \n\n1. Define a variable `y` that isolates the target variable (`generous`).\n2. Define a variable `X` that isolates the features.\n3. Split the data into training and testing sets. Put 20% of the samples into the test set, stratify the data, and set the random state.","metadata":{"id":"Nx41bVxX89Fe"}},{"cell_type":"code","source":"# Isolate target variable (y)\ny = df2['generous']\n\n# Isolate the features (X)\nX = df2.drop('generous', axis=1)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)","metadata":{"executionInfo":{"elapsed":25703,"status":"aborted","timestamp":1669031931672,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"qLbapbSWDUL-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **Random forest**\n\nBegin with using `GridSearchCV` to tune a random forest model.\n\n1. Instantiate the random forest classifier `rf` and set the random state.\n\n2. Create a dictionary `cv_params` of any of the following hyperparameters and their corresponding values to tune. The more you tune, the better your model will fit the data, but the longer it will take. \n - `max_depth`  \n - `max_features`  \n - `max_samples` \n - `min_samples_leaf`  \n - `min_samples_split`\n - `n_estimators`  \n\n3. Define a set `scoring` of scoring metrics for GridSearch to capture (precision, recall, F1 score, and accuracy).\n\n4. Instantiate the `GridSearchCV` object `rf1`. Pass to it as arguments:\n - estimator=`rf`\n - param_grid=`cv_params`\n - scoring=`scoring`\n - cv: define the number of you cross-validation folds you want (`cv=_`)\n - refit: indicate which evaluation metric you want to use to select the model (`refit=_`)\n\n **Note:** `refit` should be set to `'f1'`.<font/>\n</details>\n","metadata":{"id":"vynZs5het1b_"}},{"cell_type":"code","source":"# 1. Instantiate the random forest classifier\nrf = RandomForestClassifier(random_state=42)\n\n# 2. Create a dictionary of hyperparameters to tune \n# Note that this example only contains 1 value for each parameter for simplicity,\n# but you should assign a dictionary with ranges of values\ncv_params = {'max_depth': [None],\n             'max_features': [1.0],\n             'max_samples': [0.7],\n             'min_samples_leaf': [1],\n             'min_samples_split': [2],\n             'n_estimators': [300]\n             }\n\n# 3. Define a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1'}\n\n# 4. Instantiate the GridSearchCV object\nrf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"executionInfo":{"elapsed":25701,"status":"aborted","timestamp":1669031931672,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"Vj5rJWOv5O3d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now fit the model to the training data.<br>  \n\n**_Note_**: _Depending on how many options you include in your search grid and the number of cross-validation folds you select, this could take a very long time&mdash;even hours. If you use 4-fold validation and include only one possible value for each hyperparameter and grow 300 trees to full depth, it should take about 5 minutes. If you add another value for GridSearch to check for, say, `min_samples_split` (so all hyperparameters now have 1 value except for `min_samples_split`, which has 2 possibilities), it would double the time to ~10 minutes. Each additional parameter would approximately double the time._ ","metadata":{"id":"Wv_WvRA1RqTl"}},{"cell_type":"code","source":"%%time\nrf1.fit(X_train, y_train)","metadata":{"executionInfo":{"elapsed":25701,"status":"aborted","timestamp":1669031931673,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"OXuBiTGi5ZHn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIf you get a warning that a metric is 0 due to no predicted samples, think about how many features you're sampling with `max_features`. How many features are in the dataset? How many are likely predictive enough to give good predictions within the number of splits you've allowed (determined by the `max_depth` hyperparameter)? Consider increasing `max_features`.\n","metadata":{"id":"5wHi_YJduQOH"}},{"cell_type":"markdown","source":"If you want, use `pickle` to save your models and read them back in. This can be particularly helpful when performing a search over many possible hyperparameter values.","metadata":{"id":"ChZsXw2sksDF"}},{"cell_type":"code","source":"import pickle \n\n# Define a path to the folder where you want to save the model\npath = '/home/jovyan/work/'","metadata":{"executionInfo":{"elapsed":25699,"status":"aborted","timestamp":1669031931673,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"YtAgrH0zy4CE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_pickle(path, model_object, save_name:str):\n    '''\n    save_name is a string.\n    '''\n    with open(path + save_name + '.pickle', 'wb') as to_write:\n        pickle.dump(model_object, to_write)","metadata":{"executionInfo":{"elapsed":25699,"status":"aborted","timestamp":1669031931674,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"6JsLR2-uy9p1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_pickle(path, saved_model_name:str):\n    '''\n    saved_model_name is a string.\n    '''\n    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n        model = pickle.load(to_read)\n\n        return model","metadata":{"executionInfo":{"elapsed":25694,"status":"aborted","timestamp":1669031931674,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"eE7GMb82zRsT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examine the best average score across all the validation folds. ","metadata":{"id":"QIaRiZW4hf-6"}},{"cell_type":"code","source":"# Examine best score\nrf1.best_score_","metadata":{"executionInfo":{"elapsed":25686,"status":"aborted","timestamp":1669031931675,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"29kGUegqhviL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examine the best combination of hyperparameters.","metadata":{"id":"heGb51fHh3E5"}},{"cell_type":"code","source":"rf1.best_params_","metadata":{"executionInfo":{"elapsed":25682,"status":"aborted","timestamp":1669031931676,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"FjgXbO7Kh8is","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the `make_results()` function to output all of the scores of your model. Note that it accepts three arguments. ","metadata":{"id":"qZZnem5yiAau"}},{"cell_type":"markdown","source":"\nTo learn more about how this function accesses the cross-validation results, refer to the [`GridSearchCV` scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) for the `cv_results_` attribute.\n","metadata":{"id":"GeW48TS742jN"}},{"cell_type":"code","source":"def make_results(model_name:str, model_object, metric:str):\n    '''\n    Arguments:\n    model_name (string): what you want the model to be called in the output table\n    model_object: a fit GridSearchCV object\n    metric (string): precision, recall, f1, or accuracy\n\n    Returns a pandas df with the F1, recall, precision, and accuracy scores\n    for the model with the best mean 'metric' score across all validation folds.\n    '''\n\n    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n    metric_dict = {'precision': 'mean_test_precision',\n                 'recall': 'mean_test_recall',\n                 'f1': 'mean_test_f1',\n                 'accuracy': 'mean_test_accuracy',\n                 }\n\n    # Get all the results from the CV and put them in a df\n    cv_results = pd.DataFrame(model_object.cv_results_)\n\n    # Isolate the row of the df with the max(metric) score\n    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n\n    # Extract Accuracy, precision, recall, and f1 score from that row\n    f1 = best_estimator_results.mean_test_f1\n    recall = best_estimator_results.mean_test_recall\n    precision = best_estimator_results.mean_test_precision\n    accuracy = best_estimator_results.mean_test_accuracy\n\n    # Create table of results\n    table = pd.DataFrame({'model': [model_name],\n                        'precision': [precision],\n                        'recall': [recall],\n                        'F1': [f1],\n                        'accuracy': [accuracy],\n                        },\n                       )\n\n    return table","metadata":{"executionInfo":{"elapsed":25680,"status":"aborted","timestamp":1669031931676,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"u-UodWEOedxz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call 'make_results()' on the GridSearch object\nresults = make_results('RF CV', rf1, 'f1')\nresults","metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1669031931844,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"qAYb2QigiT_h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is an acceptable model across the board. Typically scores of 0.65 or better are considered acceptable, but this is always dependent on your use case. Optional: try to improve the scores. It's worth trying, especially to practice searching over different hyperparameters.\n\n\nFor example, if the available values for `min_samples_split` were [2, 3, 4] and GridSearch identified the best value as 4, consider trying [4, 5, 6] this time.\n","metadata":{"id":"SB-yhW9uu7dO"}},{"cell_type":"markdown","source":"Use your model to predict on the test data. Assign the results to a variable called `rf_preds`.\n\nYou cannot call `predict()` on the GridSearchCV object directly. You must call it on the `best_estimator_`.\n\n\nNOTE: For this project, you will use several models to predict on the test data. Remember that this decision comes with a trade-off. What is the benefit of this? What is the drawback?\n\n**Response:**\n\nThe benefit of using multiple models to predict on the test data is that you can compare models using data that was not used to train/tune hyperparameters. This reduces the risk of selecting a model based on how well it fit the training data.\n\nThe drawback of using the final test data to select a model is that, by using the unseen data to make a decision about which model to use, you no longer have a truly unbiased idea of how your model would be expected to perform on new data. In this case, think of final model selection as another way of \"tuning\" your model.","metadata":{}},{"cell_type":"code","source":"# Get scores on test data\nrf_preds = rf1.best_estimator_.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the below `get_test_scores()` function you will use to output the scores of the model on the test data.","metadata":{}},{"cell_type":"code","source":"def get_test_scores(model_name:str, preds, y_test_data):\n    '''\n    Generate a table of test scores.\n\n    In:\n    model_name (string): Your choice: how the model will be named in the output table\n    preds: numpy array of test predictions\n    y_test_data: numpy array of y_test data\n\n    Out:\n    table: a pandas df of precision, recall, f1, and accuracy scores for your model\n    '''\n    accuracy = accuracy_score(y_test_data, preds)\n    precision = precision_score(y_test_data, preds)\n    recall = recall_score(y_test_data, preds)\n    f1 = f1_score(y_test_data, preds)\n\n    table = pd.DataFrame({'model': [model_name],\n                        'precision': [precision],\n                        'recall': [recall],\n                        'F1': [f1],\n                        'accuracy': [accuracy]\n                        })\n\n    return table","metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1669031931845,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"ycwjBHJjiT9J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Use the `get_test_scores()` function to generate the scores on the test data. Assign the results to `rf_test_scores`.\n2. Call `rf_test_scores` to output the results.","metadata":{"id":"FDRAL7zQx21J"}},{"cell_type":"markdown","source":"###### RF test results","metadata":{}},{"cell_type":"code","source":"# Get scores on test data\nrf_test_scores = get_test_scores('RF test', rf_preds, y_test)\nresults = pd.concat([results, rf_test_scores], axis=0)\nresults","metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1669031931845,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"Iil1LjabiT5x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question:** How do your test results compare to your validation results?","metadata":{}},{"cell_type":"markdown","source":"**Response:** All scores increased by at most ~0.02.","metadata":{"id":"E4JiP5VRz2un"}},{"cell_type":"markdown","source":"##### **XGBoost**\n\n Try to improve your scores using an XGBoost model.\n\n1. Instantiate the XGBoost classifier `xgb` and set `objective='binary:logistic'`. Also set the random state.\n\n2. Create a dictionary `cv_params` of the following hyperparameters and their corresponding values to tune:\n - `max_depth`\n - `min_child_weight`\n - `learning_rate`\n - `n_estimators`\n\n3. Define a set `scoring` of scoring metrics for grid search to capture (precision, recall, F1 score, and accuracy).\n\n4. Instantiate the `GridSearchCV` object `xgb1`. Pass to it as arguments:\n - estimator=`xgb`\n - param_grid=`cv_params`\n - scoring=`scoring`\n - cv: define the number of cross-validation folds you want (`cv=_`)\n - refit: indicate which evaluation metric you want to use to select the model (`refit='f1'`)","metadata":{}},{"cell_type":"code","source":"# 1. Instantiate the XGBoost classifier\nxgb = XGBClassifier(objective='binary:logistic', random_state=0)\n\n# 2. Create a dictionary of hyperparameters to tune\n# Note that this example only contains 1 value for each parameter for simplicity,\n# but you should assign a dictionary with ranges of values\ncv_params = {'learning_rate': [0.1],\n             'max_depth': [8],\n             'min_child_weight': [2],\n             'n_estimators': [500]\n             }\n\n# 3. Define a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1'}\n\n# 4. Instantiate the GridSearchCV object\nxgb1 = GridSearchCV(xgb, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1669031931846,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"dE6oXEJJiT2R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now fit the model to the `X_train` and `y_train` data.","metadata":{}},{"cell_type":"code","source":"%%time\nxgb1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the best score from this model.","metadata":{}},{"cell_type":"code","source":"# Examine best score\nxgb1.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the best parameters.","metadata":{"id":"2bB-QyGz0RcU"}},{"cell_type":"code","source":"# Examine best parameters\nxgb1.best_params_","metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1669031931846,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"JiLja3YViTzj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### XGB CV results","metadata":{}},{"cell_type":"markdown","source":"Use the `make_results()` function to output all of the scores of your model. Note that it accepts three arguments. ","metadata":{"id":"eTE2QdNP0eEP"}},{"cell_type":"code","source":"# Call 'make_results()' on the GridSearch object\nxgb1_cv_results = make_results('XGB CV', xgb1, 'f1')\nresults = pd.concat([results, xgb1_cv_results], axis=0)\nresults","metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1669031931847,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"L4TSYXJWiTxs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use your model to predict on the test data. Assign the results to a variable called `xgb_preds`.\n","metadata":{"id":"wR1QdIAX1dKX"}},{"cell_type":"code","source":"# Get scores on test data\nxgb_preds = xgb1.best_estimator_.predict(X_test)","metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1669031931847,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"5Y2giCN32Dwc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### XGB test results\n\n1. Use the `get_test_scores()` function to generate the scores on the test data. Assign the results to `xgb_test_scores`.\n2. Call `xgb_test_scores` to output the results.","metadata":{"id":"0EnxPK7R1C5Q"}},{"cell_type":"code","source":"# Get scores on test data\nxgb_test_scores = get_test_scores('XGB test', xgb_preds, y_test)\nresults = pd.concat([results, xgb_test_scores], axis=0)\nresults","metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1669031931848,"user":{"displayName":"Jim McCoy","userId":"05540602321492626965"},"user_tz":360},"id":"g7jShC2TiTvx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Response:** The F<sub>1</sub> score is ~0.01 lower than the random forest model. Both models are acceptable, but the random forest model is the champion.","metadata":{"id":"xZjClJnncJ-j"}},{"cell_type":"markdown","source":"Plot a confusion matrix of the champion model's predictions on the test data.","metadata":{}},{"cell_type":"code","source":"# Generate array of values for confusion matrix\ncm = confusion_matrix(y_test, rf_preds, labels=rf1.classes_)\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                             display_labels=rf1.classes_, \n                             )\ndisp.plot(values_format='');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Response:** The model is almost twice as likely to predict a false positive than it is to predict a false negative. Therefore, type I errors are more common. This is less desirable, because it's better for a driver to be pleasantly surprised by a generous tip when they weren't expecting one than to be disappointed by a low tip when they were expecting a generous one. However, the overall performance of this model is satisfactory. ","metadata":{}},{"cell_type":"markdown","source":"##### Feature importance\n\nUse the `feature_importances_` attribute of the best estimator object to inspect the features of your final model. You can then sort them and plot the most important ones.","metadata":{}},{"cell_type":"code","source":"importances = rf1.best_estimator_.feature_importances_\nrf_importances = pd.Series(importances, index=X_test.columns)\nrf_importances = rf_importances.sort_values(ascending=False)[:15]\n\nfig, ax = plt.subplots(figsize=(8,5))\nrf_importances.plot.bar(ax=ax)\nax.set_title('Feature importances')\nax.set_ylabel('Mean decrease in impurity')\nfig.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PACE: **Execute**\n","metadata":{"id":"_HGsWfEOeWPm"}},{"cell_type":"markdown","source":"### **Task 4. Conclusion**\n\nIn this step, use the results of the models above to formulate a conclusion. Consider the following questions:\n\n**Responses:**\n1. **Would you recommend using this model? Why or why not?**  \nYes, this is model performs acceptably. Its F<sub>1</sub> score was 0.7235 and it had an overall accuracy of 0.6865. It correctly identified ~78% of the actual responders in the test set, which is 48% better than a random guess. It may be worthwhile to test the model with a select group of taxi drivers to get feedback.\n\n2. **What was your highest scoring model doing? Can you explain how it was making predictions?**   \nUnfortunately, random forest is not the most transparent machine learning algorithm. We know that `VendorID`, `predicted_fare`, `mean_duration`, and `mean_distance` are the most important features, but we don't know how they influence tipping. This would require further exploration. It is interesting that `VendorID` is the most predictive feature. This seems to indicate that one of the two vendors tends to attract more generous customers. It may be worth performing statistical tests on the different vendors to examine this further.\n\n3. **Are there new features that you can engineer that might improve model performance?**   \nThere are almost always additional features that can be engineered, but hopefully the most obvious ones were generated during the first round of modeling. In our case, we could try creating three new columns that indicate if the trip distance is short, medium, or far. We could also engineer a column that gives a ratio that represents (the amount of money from the fare amount to the nearest higher multiple of $5) / fare amount. For example, if the fare were \\\\$12, the value in this column would be 0.25, because \\\\$12 to the nearest higher multiple of \\\\$5 (\\\\$15) is \\\\$3, and \\\\$3 divided by \\\\$12 is 0.25. The intuition for this feature is that people might be likely to simply round up their tip, so journeys with fares with values just under a multiple of \\\\$5 may have lower tip percentages than those with fare values just over a multiple of \\\\$5. We could also do the same thing for fares to the nearest \\\\$10.\n\n\n$$round5\\_ratio = \\frac{amount\\ of\\ money\\ from\\ the\\ fare\\ amount\\ to\\ the\\ nearest\\ higher\\ multiple\\ of\\ $5}{fare\\ amount}$$   \n\n\n  $$ = \\frac{5 - (fare\\ mod\\ 5)}{fare\\ amount}$$  \n\n\n\n4. **What features would you want to have that would likely improve the performance of your model?**   \nIt would probably be very helpful to have past tipping behavior for each customer. It would also be valuable to have accurate tip values for customers who pay with cash.\nIt would be helpful to have a lot more data. With enough data, we could create a unique feature for each pickup/dropoff combination.\n\n\nRemember, sometimes your data simply will not be predictive of your chosen target. This is common. Machine learning is a powerful tool, but it is not magic. If your data does not contain predictive signal, even the most complex algorithm will not be able to deliver consistent and accurate predictions. Do not be afraid to draw this conclusion. Even if you cannot use the model to make strong predictions, was the work done in vain? What insights can you report back to stakeholders?","metadata":{"id":"ill21hQ4ej9-"}}]}